[{"id":0,"href":"/docs/active-projects/asem/project/","title":"ASEM","section":"Asem","content":" Automated Segmentation of cellular substructures in Electron Microscopy (ASEM) # This repository contains the segmentation pipeline described in\nBenjamin Gallusser, Giorgio Maltese, Giuseppe Di Caprio et al.Deep neural network automated segmentation of cellular structures in volume electron microscopy,Journal of Cell Biology, 2022.\nPlease cite the publication if you are using this code in your research.\nOur semi-automated annotation tool from the same publication is available at https://github.com/kirchhausenlab/gc_segment.\nTable of Contents # Installation Optional: Download our data Prepare your own data for prediction Prediction Prepare your own ground truth annotations for fine-tuning or training Fine-Tuning Training Installation # This package is written for machines with either a Linux or a MacOS operating system.\nThis README was written to work with the bash console. If you want to use zsh (default on newer versions of MacOS) or any other console, please make sure that you adapt things accordingly.\nNewer versions of MacOS (Catalina or newer): the following commands work correctly if you run the Terminal under Rosetta. In Finder, go to Applications/Utilities, right click on Terminal, select Get Info, tick Open using Rosetta.\n1. Install anaconda for creating a conda python environment. # Open a terminal window and download anaconda.\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Optional: In case of permission issues, run\nchmod +x Miniconda3-latest-Linux-x86_64.sh` Install anaconda.\nbash Miniconda3-latest-Linux-x86_64.sh Type conda to check whether the installation worked, if not try to reload your .bashrc file with source ~/.bashrc.\n2. Clone the main repository. # git clone https://github.com/kirchhausenlab/incasem.git ~/incasem 3. Create a new anaconda python environment. # conda create -n incasem --no-default-packages python=3.8 4. Pip-install the incasem package contained in this repository into the environment. # Activate the new environment.\nconda activate incasem Install\npip install -e ./incasem 5. Install pytorch as outlined here. # 6. Install our neuroglancer scripts # pip install git+https://github.com/kirchhausenlab/funlib.show.neuroglancer.git@more_scripts_v2#egg=funlib.show.neuroglancer 7. Set up the experiment tracking databases for training and prediction # If not already installed on your system (check by running mongod), install MongoDB.\nStart up the MongoDB service (refer to documentation):\non Ubuntu: bash sudo service mongod start on MacOS (assuming that you have installed mongodb via homebrew): brew services start mongodb-community Run\ncd ~/incasem; python download_models.py 9. Install Omniboard for viewing the experiment databases # Install Nodeenv.\npip install nodeenv Create a new node.js environment (and thereby install node.js, if not already installed).\nThis may take a while.\ncd ~; nodeenv omniboard_environment Activate the environment.\nsource omniboard_environment/bin/activate Install omniboard.\nThis may take a while.\nnpm install -g omniboard Optional: Download our data # The datasets in the publication are available in an AWS bucket and can be downloaded with the quilt3 API.\n1. Download an example dataset from the AWS bucket: cell 6 # Navigate to ~/incasem/data:\ncd ~/incasem/data Open a python session and run the following lines.\nIt may take a while until the download starts. Expected download speed is \u0026gt;= 2MB/s.\nimport quilt3 b = quilt3.Bucket(\u0026#34;s3://asem-project\u0026#34;) # download b.fetch(\u0026#34;datasets/cell_6/cell_6_example.zarr/\u0026#34;, \u0026#34;cell_6/cell_6.zarr/\u0026#34;) We provide all datasets as 2d .tiff images as well as in .zarr format, which is more suitable for deep learning on 3D images. Above we only downloaded the .zarr format.\n2. Explore a dataset # Example: Cell 6 raw electron microscopy data, Endoplasmic Reticulum prediction and corresponding Endoplasmic Reticulum ground-truth annotation.\nneuroglancer -f cell_6/cell_6.zarr -d volumes/raw_equalized_0.02 volumes/predictions/er/segmentation volumes/labels/er Navigate to position 520, 1164, 2776 (z,y,x) to focus on the Endoplasmic Reticulum predictions. You can simply overwrite the coordinates on the top left to do so.\nIf you are not familiar with inspecting 3D data with neuroglancer, you might want to have a look at this video tutorial.\nNote: neuroglancer might not work in Safari. In this case, simply copy the link given by neuroglancer to Chrome or Firefox.\nPrepare your own data for prediction # We assume that the available 3d data is stored as a sequence of 2d .tif images in a directory.\n0. Copy your data into the project directory # cp -r old/data/location ~/incasem/data/my_new_data 1. Go to the 01_data_formatting directory # cd ~/incasem/scripts/01_data_formatting 2. Activate the python environment # In case you have not installed the python environment yet, refer to the installation instructions.\nBefore running python scripts, activate the incasem environment\nconda activate incasem 3. Conversion from TIFF to zarr format # Convert the sequence of .tif images (3D stack) to .zarr format.\npython 00_image_sequences_to_zarr.py -i ~/incasem/data/my_new_data -f ~/incasem/data/my_new_data.zarr To obtain documentation on how to use a script, run python \u0026lt;script_name\u0026gt;.py -h.\nIf your datasets is hundreds of GB in size, try using the conversion script 01_image_sequences_to_zarr_with_dask.py. You will need to install a different conda environment to work with dask, details directly in the script.\npython 01_image_sequences_to_zarr_with_dask.py -i ~/incasem/data/my_new_data -f ~/incasem/data/my_new_data.zarr -d volumes/raw --resolution 5 5 5 4. Equalize intensity histogram of the data # Equalize the raw data with CLAHE (Contrast limited adaptive histogram equalization). The default clip limit is 0.02.\npython 40_equalize_histogram.py -f ~/incasem/data/my_new_data.zarr -d volumes/raw -o volumes/raw_equalized_0.02 5. Inspect the converted data with neuroglancer: # neuroglancer -f ~/incasem/data/my_new_data.zarr -d /volumes/raw Refer to our instructions on how to use neuroglancer.\nPrediction # 1. Create a data configuration file # For running a prediction you need to create a configuration file in JSON format that specifies which data should be used. Here is an example, also available at ~/incasem/scripts/03_predict/data_configs/example_cell6.json:\n{ \u0026#34;Cell_6_example_roi_nickname\u0026#34;: { \u0026#34;file\u0026#34;: \u0026#34;cell_6/cell_6.zarr\u0026#34;, \u0026#34;offset\u0026#34;: [400, 926, 2512], \u0026#34;shape\u0026#34;: [241, 476, 528], \u0026#34;voxel_size\u0026#34;: [5, 5, 5], \u0026#34;raw\u0026#34;: \u0026#34;volumes/raw_equalized_0.02\u0026#34; } } offset and shape are specified in voxels and in z, y, x format. They have to outline a region of interest (ROI) that lies within the total available ROI of the dataset (as defined in .zarray and .zattrs files of each zarr volume).\nNote that the offset in each .zattr file is defined in nanometers, while the shape in .zarray is defined in voxels.\nWe assume the data to be in ~/incasem/data, as defined here.\n2. Choose a model # We provide the following pre-trained models:\nFor FIB-SEM data prepared by chemical fixation, 5x5x5 nm3 resolution: Mitochondria (model ID 1847) Golgi Apparatus (model ID 1837) Endoplasmic Reticulum (model ID 1841) For FIB-SEM data prepared by high-pressure freezing, 4x4x4 nm3 resolution: Mitochondria (model ID 1675) Endoplasmic Reticulum (model ID 1669) For FIB-SEM data prepared by high-pressure freezing, 5x5x5 nm3 resolution: Clathrin-Coated Pits (model ID 1986) Nuclear Pores (model ID 2000) A checkpoint file for each of these models is stored in ~/incasem/models/pretrained_checkpoints/.\nOptional: For detailed information about the trained modes, refer to the database downloaded above: # Activate the omniboard environment.\nsource ~/omniboard_environment/bin/activate Run\nomniboard -m localhost:27017:incasem_trainings and paste localhost:9000 into your browser.\n3. Run the prediction # Cell 6 has been prepared by chemical fixation and we will generate predictions for Endoplasmic Reticulum in this example, using model ID 1841. In the prediction scripts folder,\ncd ~/incasem/scripts/03_predict Run\npython predict.py --run_id 1841 --name example_prediction_cell6_ER with config_prediction.yaml \u0026#39;prediction.data=data_configs/example_cell6.json\u0026#39; \u0026#39;prediction.checkpoint=../../models/pretrained_checkpoints/model_checkpoint_1841_er_CF.pt\u0026#39; Note that we need to specify which model to use twice:\n--run_id 1841 to load the appropriate settings from the models database. 'prediction.checkpoint=../../models/pretrained_checkpoints/model_checkpoint_1841_er_CF.pt' to pass the path to the checkpoint file. You can check the status of the prediction in omniboard:\nomniboard -m localhost:27017:incasem_predictions Optional: # If you have corresponding ground truth annotations, create a metric exclusion zone as described below. For the example of predicting Endoplasmic Reticulum in cell 6 from above, put the metric exclusion zone in cell_6/cell_6.zarr/volumes/metric_masks/er and adapt data_configs/example_cell6.json to:\n{ \u0026#34;Cell_6_example_roi_nickname\u0026#34;: { \u0026#34;file\u0026#34;: \u0026#34;cell_6/cell_6.zarr\u0026#34;, \u0026#34;offset\u0026#34;: [400, 926, 2512], \u0026#34;shape\u0026#34;: [241, 476, 528], \u0026#34;voxel_size\u0026#34;: [5, 5, 5], \u0026#34;raw\u0026#34;: \u0026#34;volumes/raw_equalized_0.02\u0026#34;, \u0026#34;metric_masks\u0026#34;: [\u0026#34;volumes/metric_masks/er\u0026#34;], \u0026#34;labels\u0026#34;: { \u0026#34;volumes/labels/er\u0026#34;: 1 } } } Now run\npython predict.py --run_id 1841 --name example_prediction_cell6_ER_with_GT with config_prediction.yaml \u0026#39;prediction.log_metrics=True\u0026#39; \u0026#39;prediction.data=data_configs/example_cell6.json\u0026#39; \u0026#39;prediction.checkpoint=../../models/pretrained_checkpoints/model_checkpoint_1841_er_CF.pt\u0026#39; , which will print an F1 score for the generated prediction given the ground truth annotations (labels).\n4. Visualize the prediction # Every prediction is stored with a unique identifier (increasing number). If the example above was your first prediction run, you will see a folder ~/incasem/data/cell_6/cell_6.zarr/volumes/predictions/train_1841/predict_0001/segmentation. To inspect these predictions, together with the corresponding EM data and ground truth, use the following command:\nneuroglancer -f ~/incasem/data/cell_6/cell_6.zarr -d volumes/raw_equalized_0.02 volumes/labels/er volumes/predictions/train_1841/predict_0001/segmentation 5. Convert the prediction to \u0026lsquo;TIFF\u0026rsquo; format # Run cd ~/incasem/scripts/04_postprocessing to access the postprocessing scripts.\nNow adapt and execute the conversion command below. In this example command, we assume that we have used model ID 1841 to generate Endoplasmic Reticulum predictions for a subset of cell 6, and the automatically assigned prediction ID is 0001.\npython 20_convert_zarr_to_image_sequence.py --filename ~/incasem/data/cell_6/cell_6.zarr --datasets volumes/predictions/train_1841/predict_0001/segmentation --out_directory ~/incasem/data/cell_6 --out_datasets example_er_prediction You can open the resulting TIFF stack for example in ImageJ. Note that since we only made predictions on a subset of cell 6, the prediction TIFF stack is smaller than the raw data TIFF stack.\nPrepare your own ground truth annotations for fine-tuning or training # Example: Endoplasmic reticulum (ER) annotations.\nWe assume that the available 3d pixelwise annotations are stored as a sequence of 2d .tif images in a directory and that the size of each .tif annotation image matches the size of the corresponding electron microscopy .tif image.\nFurthermore, we assume that you have already prepared the corresponding electron microscopy images as outlined above.\nThe minimal block size that our training pipeline is set up to process is (204, 204, 204) voxels.\n0. Copy the annotation data into the project directory # cp -r old/annotations/location ~/incasem/data/my_new_er_annotations 1. Go to the 01_data_formatting directory # cd ~/incasem/scripts/01_data_formatting 2. Activate the python environment # In case you have not installed the python environment yet, refer to the installation instructions.\nBefore running python scripts, activate the incasem environment\nconda activate incasem 3. Conversion from TIFF to zarr format # Convert the sequence of .tif annotations (3D stack) to .zarr format. In this example, we use\npython 00_image_sequences_to_zarr.py -i ~/incasem/data/my_new_er_annotations -f ~/incasem/data/my_new_data.zarr -d volumes/labels/er --dtype uint32 We assume the .tif file names are in the format name_number.tif, as encapsulated by the default regular expression .*_(\\d+).*\\.tif$. If you want to change it, add -r your_regular_expression to the line above.\nIf your datasets is hundreds of GB in size, try using the conversion script 01_image_sequences_to_zarr_with_dask.py. You will need to install a different conda environment to work with dask, details directly in the script.\npython 01_image_sequences_to_zarr_with_dask.py -i ~/incasem/data/my_new_er_annotations -f ~/incasem/data/my_new_data.zarr -d volumes/labels_er --resolution 5 5 5 --dtype uint32 Inspect the converted data with neuroglancer:\nneuroglancer -f ~/incasem/data/my_new_data.zarr -d volumes/raw volumes/labels/er Refer to our instructions on how to use neuroglancer.\nIf the position of the labels is wrong, you can correct the offset by directly editing the dataset attributes file on disk:\ncd ~/incasem/data/my_new_data.zarr/volumes/labels/er vim .zattrs In this file the offset is expressed in nanometers instead of voxels. So if the voxel size is (5,5,5) nm, you need to multiply the previous coordinates (z,y,x) by 5.\n4. Create a metric exclusion zone # We create a mask that will be used to calculate the F1 score for predictions, e.g. in the periodic validation during training. This mask, which we refer to as exclusion zone, simply sets the pixels at the object boundaries to 0, as we do not want that small errors close to the object boundaries affect the overall prediction score.\nWe suggest the following exclusion zones in voxels:\nmito: 4 --exclude_voxels_inwards 4 --exclude_voxels_outwards 4 golgi: 2 --exclude_voxels_inwards 2 --exclude_voxels_outwards 2 ER: 2 --exclude_voxels_inwards 2 --exclude_voxels_outwards 2 NP (nuclear pores): 1 --exclude_voxels_inwards 1 --exclude_voxels_outwards 1 CCP (coated pits): 1 --exclude_voxels_inwards 2 --exclude_voxels_outwards 2 For our example with Endoplasmic Reticulum annotations, we run\npython 60_create_metric_mask.py -f ~/incasem/data/my_new_data.zarr -d volumes/labels/er --out_dataset volumes/metric_masks/er --exclude_voxels_inwards 2 --exclude_voxels_outwards 2 Fine-Tuning # If the prediction quality on a new target cell when using one of our pre-trained models is not satisfactory, you can finetune the model with a very small amount of ground truth from that target cell.\nThis is an example based on our datasets, which are publicly available in .zarr format via Amazon Web Services. We will fine-tune the mitochondria model ID 1847, which was trained on data from cells 1 and 2, with a small amount of additional data from cell 3.\n0. Download training data # If you haven\u0026rsquo;t done so before, download cell_3 from our published datasets as outlined in the section Download our data.\n1. Create a fine-tuning data configuration file # For fine-tuning a model you need to create a configuration file in JSON format that specifies which data should be used. Here is an example, also available at ~/incasem/scripts/02_train/data_configs/example_finetune_mito.json:\n{ \u0026#34;cell_3_finetune_mito\u0026#34;: { \u0026#34;file\u0026#34;: \u0026#34;cell_3/cell_3.zarr\u0026#34;, \u0026#34;offset\u0026#34;: [700, 2000, 6200], \u0026#34;shape\u0026#34;: [250, 250, 250], \u0026#34;voxel_size\u0026#34;: [5, 5, 5], \u0026#34;raw\u0026#34;: \u0026#34;volumes/raw_equalized_0.02\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;volumes/labels/mito\u0026#34;: 1 } } } Refer to the section Training for a detailed walk-through of such a configuration file.\n2. Launch the fine-tune training # In the training scripts folder,\ncd ~/incasem/scripts/02_train run\npython train.py --name example_finetune --start_from 1847 ~/incasem/models/pretrained_checkpoints/model_checkpoint_1847_mito_CF.pt with config_training.yaml training.data=data_configs/example_finetune_mito.json validation.data=data_configs/example_finetune_mito.json torch.device=0 training.iterations=15000 Note that since we do not have extra validation data on the target cell 3, we simply pass the training data configuration file to define a dummy validation dataset.\n3. Observe the training # Each training run logs information to disk and to the training database, which can be inspected using Omniboard. The log files on disk are stored in ~/incasem/training_runs.\nTensorboard # To monitor the training loss in detail, open tensorboard:\ntensorboard --logdir=~/incasem/training_runs/tensorboard Omniboard (training database) # To observe the training and validation F1 scores, as well as the chosen experiment configuration, we use Omniboard.\nActivate the omniboard environment:\nsource ~/omniboard_environment/bin/activate Run\nomniboard -m localhost:27017:incasem_trainings and paste localhost:9000 into your browser.\n4. Pick a fine-tuned model for prediction # Since we usually do not have any ground truth on the target cell that we fine-tuned for, we cannot rigorously pick the best model iteration.\nWe find that for example with ground truth in a 2 um3 region of interest, typically after 5,000 - 10,000 iterations the fine-tuning has converged. The training loss (visible in tensorboard) can serve as a proxy for picking a model iteration in said interval.\nNow you can use the fine-tuned model to generate predictions on the new target cell, as described in the section Prediction.\nTraining # This is an example based on our datasets, which are publicly available in .zarr format via Amazon Web Services.\n0. Download training data # Download cell_1 and cell_2 from our published datasets as outlined in the section Download our data.\n1. Prepare the data # We create a mask that will be used to calculate the F1 score for predictions, e.g. in the periodic validation during training. This mask, which we refer to as exclusion zone, simply sets the pixels at the object boundaries to 0, as we do not want that small errors close to the object boundaries affect the overall prediction score.\nFor our example with Endoplasmic Reticulum annotations on cell_1 and cell_2, we run (from the data formatting directory):\npython 60_create_metric_mask.py -f ~/incasem/data/cell_1/cell_1.zarr -d volumes/labels/er --out_dataset volumes/metric_masks/er --exclude_voxels_inwards 2 --exclude_voxels_outwards 2 and\npython 60_create_metric_mask.py -f ~/incasem/data/cell_2/cell_2.zarr -d volumes/labels/er --out_dataset volumes/metric_masks/er --exclude_voxels_inwards 2 --exclude_voxels_outwards 2 2. Create a training data configuration file # For running a training you need to create a configuration file in JSON format that specifies which data should be used. Here is an example, also available at ~/incasem/scripts/02_train/data_configs/example_train_er.json:\nWe assume the data to be in ~/incasem/data, as defined here.\n{ \u0026#34;cell_1_er\u0026#34;: { \u0026#34;file\u0026#34;: \u0026#34;cell_1/cell_1.zarr\u0026#34;, \u0026#34;offset\u0026#34;: [150, 120, 1295], \u0026#34;shape\u0026#34;: [600, 590, 1350], \u0026#34;voxel_size\u0026#34;: [5, 5, 5], \u0026#34;raw\u0026#34;: \u0026#34;volumes/raw_equalized_0.02\u0026#34;, \u0026#34;metric_masks\u0026#34;: [\u0026#34;volumes/metric_masks/er\u0026#34;], \u0026#34;labels\u0026#34;: { \u0026#34;volumes/labels/er\u0026#34;: 1 } }, \u0026#34;cell_2_er\u0026#34;: { \u0026#34;file\u0026#34;: \u0026#34;cell_2/cell_2.zarr\u0026#34;, \u0026#34;offset\u0026#34;: [100, 275, 700], \u0026#34;shape\u0026#34;: [500, 395, 600], \u0026#34;voxel_size\u0026#34;: [5, 5, 5], \u0026#34;raw\u0026#34;: \u0026#34;volumes/raw_equalized_0.02\u0026#34;, \u0026#34;metric_masks\u0026#34;: [\u0026#34;volumes/metric_masks/er\u0026#34;], \u0026#34;labels\u0026#34;: { \u0026#34;volumes/labels/er\u0026#34;: 1 } } } offset and shape are specified in voxels and in z, y, x format. They have to outline a region of interest (ROI) that lies within the total available ROI of the dataset (as defined in .zarray and .zattrs files of each zarr volume).\nNote that the offset in each .zattr file is defined in nanometers, while the shape in .zarray is defined in voxels.\nAll pixels inside the ROIs that belong to the structure of interest (e.g. endoplasmic reticulum above) in such a data configuration file have to be fully annotated. Additionally, our network architecture requires a context of 47 voxels of raw EM data around each ROI.\n3. Create a validation data configuration file # Additionally, you need to create a configuration file in JSON format that specifies which data should be used for periodic validation of the model during training. Here is an example, also available at ~/incasem/scripts/02_train/data_configs/example_validation_er.json:\n{ \u0026#34;cell_1_er_validation\u0026#34;: { \u0026#34;file\u0026#34;: \u0026#34;cell_1/cell_1.zarr\u0026#34;, \u0026#34;offset\u0026#34;: [150, 120, 2645], \u0026#34;shape\u0026#34;: [600, 590, 250], \u0026#34;voxel_size\u0026#34;: [5, 5, 5], \u0026#34;raw\u0026#34;: \u0026#34;volumes/raw_equalized_0.02\u0026#34;, \u0026#34;metric_masks\u0026#34;: [\u0026#34;volumes/metric_masks/er\u0026#34;], \u0026#34;labels\u0026#34;: { \u0026#34;volumes/labels/er\u0026#34;: 1 } }, \u0026#34;cell_2_er_validation\u0026#34;: { \u0026#34;file\u0026#34;: \u0026#34;cell_2/cell_2.zarr\u0026#34;, \u0026#34;offset\u0026#34;: [300, 70, 700], \u0026#34;shape\u0026#34;: [300, 205, 600], \u0026#34;voxel_size\u0026#34;: [5, 5, 5], \u0026#34;raw\u0026#34;: \u0026#34;volumes/raw_equalized_0.02\u0026#34;, \u0026#34;metric_masks\u0026#34;: [\u0026#34;volumes/metric_masks/er\u0026#34;], \u0026#34;labels\u0026#34;: { \u0026#34;volumes/labels/er\u0026#34;: 1 } } } 4. Optional: Adapt the training configuration # The file config_training.yaml exposes a lot of parameters of the model training.\nMost importantly:\nIf you would like to use data with a different resolution, apart from specifying in the data configuration files as outlined above, you need to adapt data.voxel_size in config_training.yaml. We guide the random sampling of blocks by rejecting blocks that consist of less than a given percentage (training.reject.min_masked) of foreground voxels with a chosen probability (\u0026rsquo;training.reject.probability\u0026rsquo;). If your dataset contains a lot of background, or no background at all, you might want to adapt these parameters accordingly. 5. Launch the training # At the training scripts folder,\ncd ~/incasem/scripts/02_train run\npython train.py --name example_training with config_training.yaml training.data=data_configs/example_train_er.json validation.data=data_configs/example_validation_er.json torch.device=0 6. Observe the training # Each training run logs information to disk and to the training database, which can be inspected using Omniboard. The log files on disk are stored in ~/incasem/training_runs.\nTensorboard # To monitor the training loss in detail, open tensorboard:\ntensorboard --logdir=~/incasem/training_runs/tensorboard Omniboard (training database) # To observe the training and validation F1 scores, as well as the chosen experiment configuration, we use Omniboard.\nActivate the omniboard environment:\nsource ~/omniboard_environment/bin/activate Run\nomniboard -m localhost:27017:incasem_trainings and paste localhost:9000 into your browser.\n7. Pick a model for prediction # Using Omniboard, pick a model iteration where the validation loss and the validation F1 score have converged. Now use this model to generate predictions on new data, as described in the section Prediction.\n"},{"id":1,"href":"/docs/active-projects/cryosamba/paper/","title":"BioArchive","section":"Cryosamba","content":" CryoSamba: Self-Supervised Cryo-ET Image Denoising # Abstract # Cryogenic electron tomography (cryo-ET) has rapidly advanced as a high-resolution imaging tool for visualizing subcellular structures in 3D with molecular detail. Direct image inspection remains challenging due to inherent low signal-to-noise ratios (SNR). We introduce CryoSamba, a self-supervised deep learning-based model designed for denoising cryo-ET images. CryoSamba enhances single consecutive 2D planes in tomograms by averaging motion-compensated nearby planes through deep learning interpolation, effectively mimicking increased exposure. This approach amplifies coherent signals and reduces high-frequency noise, substantially improving tomogram contrast and SNR. CryoSamba operates on 3D volumes without needing pre-recorded images, synthetic data, labels or annotations, noise models, or paired volumes. CryoSamba suppresses high-frequency information less aggressively than do existing cryo-ET denoising methods, while retaining real information, as shown both by visual inspection and by Fourier shell correlation analysis of icosahedrally symmetric virus particles. Thus, CryoSamba enhances the analytical pipeline for direct 3D tomogram visual interpretation.\nBioArchive # Twitter Link\nBio Archive Link\nAbstract\n"},{"id":2,"href":"/docs/active-projects/cryosamba/project/","title":"CryoSamba Main","section":"Cryosamba","content":" CryoSamba: Self-Supervised Deep Volumetric Denoising for Cryo-Electron Tomography Data # For Hardware Issues please refer to the Nvidia-CUDA section # For Software Issues please refer to the Python section, Using Conda Environment # This repository contains the denoising pipeline described in the following publication:\nJose Inacio Costa-Filho, Liam Theveny, Marilina de Sautu, Tom KirchhausenCryoSamba: Self-Supervised Deep Volumetric Denoising for Cryo-Electron Tomography Data\nPlease cite this publication if you are using this code in your research. For installation, UI, and code setup questions, reach out to Arkash Jain at arkash@tklab.hms.harvard.edu\n❗WARNING❗ CryoSamba is written for machines with either a Linux or Windows operating system and a CUDA capable GPU. MacOS is not supported.\n❗WARNING❗Make sure you have CUDA drivers installed and updated on your machine. CryoSamba requires CUDA 11 to run. Support for CUDA 12 will be added soon. Refer to Instructions for Setting Up CUDA for more support.\nThese instructions are meant to be read carefully and line by line. Arbitrarily skipping lines or blindly copy-pasting code snippets will likely lead to errors.\n💥IMPORTANT💥 CryoSamba accepts as input (.mrc, .rec, .tif) single 3D files or sequences of 2D (.tif) files. For single files, the \u0026ldquo;data path\u0026rdquo; must directly reference the files, while for tif sequences the \u0026ldquo;data path\u0026rdquo; should reference the folder containing the sequence. For example, use path/to/sample_data.rec or path/to/tif_folder. Not referencing the input data properly will lead to errors.\nTable of Contents # Overview 🌐 CLI Tool 📟 Installation 🛠️ Using the tool 🔨 Terminal 💻 Installation 🛠️ Training 🚀 (OPTIONAL) Visualization with TensorBoard 📈 Inference 🔍 UI 🎮 Instructions for Setting Up CUDA FAQ Overview # CryoSamba can be run via Terminal, via a CLI Tool or via a UI (work in progress).\nIf you have access to a graphical interface, try the UI.\nIf you do not have access to a graphical interface, try the CLI Tool. This is usually the case if you want to run CryoSamba on your university\u0026rsquo;s HPC. This the recommended option!\nIf you are a more experience programmer and comfortable with using the Terminal, try our \u0026ldquo;raw\u0026rdquo; Terminal instructions. We recommend using this only if you were already able to run CryoSamba via one of the previous options.\nFinally, if you want to use CryoSamba on Windows, have a deeper understanding of the source code, change the optional parameters, or alter/use the code for your research, refer to the advanced instructions.\nOur goal is to make CryoSamba as accessible as possible.\nCLI Tool # ❗WARNING❗ These instructions require you to know how to open a terminal window on your computer, how to navigate through folders and how to copy files around.\nNote: these instructions are designed for machines with a Linux operating system. For Windows, refer to the advanced instructions.\nInstallation # Open a Terminal window and navigate to the directory where you want to save the Cryosamba code via cd /path/to/dir. Note: the expression /path/to/dir is not meant to be copy-pasted as it is. It is a general expression which means that you should replace it with the actual path to the desired directory in your own computer. Since we do not have access to your computer, we cannot give you the exact expression to copy-paste. This expression will appear several times throughout these instructions.\nIf you received CryoSamba via a zip file, run unzip path/to/Cryosamba.zip in this directory. Otherwise, run\ngit clone https://github.com/kirchhausenlab/Cryosamba.git These two options are mutually exclusive.\nOnce successfully cloned/unzipped, navigate to the scripts folder via cd path/to/Cryosamba/automate/scripts\nTo setup the environment, run:\nchmod -R u+x *.sh ./startup_script_.sh # In case of permission issues run the command below (OPTIONAL) chmod u+x ./name_of_file_ending_with.sh This creates a conda environment called cryosamba and activates it. In the future, you will need to run\nconda activate cryosamba anytime you want to run CryoSamba again.\nIn case of errors, try running conda init --all \u0026amp;\u0026amp; source ~/.bashrc in your terminal.\nUsing the Tool # From the directory CryoSamba/automate, run\npython run_cryosamba_cli.py and follow the instructions that appear on the Terminal window.\nTerminal # ❗WARNING❗ These instructions require you to know how to open a terminal window on your computer, how to navigate through folders and how to copy files around.\nNote: these instructions are designed for machines with a Linux operating system. For Windows, refer to the advanced instructions.\nInstallation # Open a Terminal window and navigate to the directory where you want to save the Cryosamba code via cd /path/to/dir. Note: the expression /path/to/dir is not meant to be copy-pasted as it is. It is a general expression which means that you should replace it with the actual path to the desired directory in your own computer. Since we do not have access to your computer, we cannot give you the exact expression to copy-paste. This expression will appear several times throughout these instructions.\nIf you received CryoSamba via a zip file, run unzip path/to/Cryosamba.zip in this directory. Otherwise, run\ngit clone https://github.com/kirchhausenlab/Cryosamba.git These two options are mutually exclusive.\nOnce successfully cloned/unzipped, navigate to the scripts folder via cd path/to/Cryosamba/automate/scripts\nTo setup the environment, run:\nchmod -R u+x *.sh ./startup_script_.sh # In case of permission issues run the command below (OPTIONAL) chmod u+x ./name_of_file_ending_with.sh This creates a conda environment called cryosamba and activates it. In the future, you will need to run\nconda activate cryosamba anytime you want to run CryoSamba again.\nIn case of errors, try running conda init --all \u0026amp;\u0026amp; source ~/.bashrc in your terminal.\nTraining # From the same directory automate/scripts, run:\n./setup_experiment_training.sh The script asks you to enter the following parameters:\nExperiment name: it will create the following folder structure cryosamba ├── runs ├── exp-name ├── train ├── inference train_config.json Data path: it must be either\nThe full path to a single (3D) .tif, .mrc or .rec file, or The full path to a folder containing a sequence of (2D) .tif files, ordered alphanumerically matching the Z-stack order. Note: Ensure consistent zero-fill in file names to maintain proper order (e.g., frame000.tif instead of frame0.tif).\nMax frame gap: explained in the manuscript. We empirically set values of 3, 6 and 10 for data at voxel resolutions of 15.72Å, 7.86Å and 2.62Å, respectively. For different resolutions, try a reasonable value interpolated from the reference ones.\nNumber of iterations: for how many iterations the training session will run\nBatch Size: number of data points passed at once to the GPUs. Higher number leads to faster training, but the whole batch might not fit into your GPU\u0026rsquo;s memory, leading to out-of-memory errors. If you\u0026rsquo;re getting these, try decreasing the batch size until they disappear. This number should be a multiple of two.\nThe generated train_config.json file will contain all parameters for training the model. If you want to change other parameters, edit the .json file directly. In advanced instructions we provide a full explanation of all config parameters.\nTo start training the model, run the command below from the same folder automate/scripts\n./train_data.sh To interrupt training, press CTRL + C. You can resume training or start from scratch if prompted.\nTraining will run until the maximum number of iterations is reached. However, training and validation losses might converge/stabilize before that, at which point you can safely halt the process and save time and money on your electricity bill. In order to monitor the losses\u0026rsquo; progress you can: 1) see the logs printed on your screen, 2) see the runtime.log file inside your training folder, or 3) visualize their plots with TensorBoard.\nThe output of the training run will be checkpoint files containing the trained model weights. There is no denoised data output at this point yet. You can used the trained model weights to run inference on your data and then get the denoised outputs.\nVisualization with TensorBoard # This step is OPTIONAL\nTensorBoard can be used to monitor the progress of the training losses.\nOpen a terminal window inside a graphical interface (e.g., XDesk). Activate the environment and run: tensorboard --logdir /path/to/dir/Cryosamba/runs/exp-name/train In a browser, open localhost:6006. Use the slider under SCALARS to smooth noisy plots. Inference # From the same directory automate/scripts, run:\n./setup_inference.sh The script asks you to enter the following parameters:\nExperiment name: same as in training (should be an existing one) Data path: same as in training Max frame gap: usually twice the value used for training TTA: whether to use Test-Time Augmentation or not (see manuscript) The generated inference_config.json file will contain all parameters for running inference. If you want to change other parameters, edit the .json file directly.\nTo start inference, run the command below from the same folder automate/scripts\n./run_inference.sh To interrupt the process, press CTRL + C. You can resume or start from scratch if prompted.\nThe output of the inference run will be the final denoised volume, located at /path/to/dir/runs/exp-name/inference. It will be either a file named result.tif, result.mrc, result.rec or a folder named result.\nYou can simply open the final denoised volume in your preferred data visualization/processing software and check how it looks like.\nUI # PLEASE WATCH THE VIDEOS IN THE GITHUB (move_to_remote_server.mp4, install_and_startup.mp4 and How_to_run.mp4 to see an end-to-end example of running CryoSamba) # From Cryosamba/automate:\npip install streamlit cd automate chmod -R u+x *.sh streamlit run main.py You can set up the environment, train models, make configs, and run inferences from here.\nInstructions for Setting Up CUDA # If it appears that your machine is unable to locate the CUDA driver, which is typically found under /usr/bin/, please follow the steps below after identifying the path for CUDA on your machine:\nSet the CUDA Home Environment Variable\nRun the following command, replacing the path with the correct one for your CUDA installation:\nexport CUDA_HOME=/path/to/your/cuda For example:\nexport CUDA_HOME=/uufs/pathto_/sys/modulefiles/CHPC-r8/Core/cuda/12.2.0.lua Ensure CUDA 11 is Installed\nVerify that CUDA version 11 is installed on your system. If it is not, please install it according to the official NVIDIA documentation.\nBy following these steps, your machine should be able to locate and use the CUDA driver, allowing you to proceed with your work.\n"},{"id":3,"href":"/docs/github/github/","title":"GitHub","section":"Git Hub","content":" Kirchhausenlab GitHub # All the code and software developed by the Kirchhausen lab can be found on our GitHub page.\nNote: You will need to be a member of the Kirchhausen lab GitHub organization to access the repositories.\n"},{"id":4,"href":"/docs/nvidia-cuda/deeplearningsetup/","title":"NVIDIA GPU and CUDA Setup Guide","section":"Nvidia Cuda","content":" Verifying NVIDIA Hardware and CUDA Installation # Check NVIDIA GPU # First, let\u0026rsquo;s verify if your system recognizes the NVIDIA GPU:\nOpen a terminal and run:\nlspci | grep -i nvidia This should display information about your NVIDIA GPU.\nRight-click on the Windows Start button and select \u0026ldquo;Device Manager\u0026rdquo;\nExpand the \u0026ldquo;Display adapters\u0026rdquo; section\nYou should see your NVIDIA GPU listed here Recent Macs don\u0026rsquo;t use NVIDIA GPUs. For older Macs:\nClick the Apple menu and select \u0026ldquo;About This Mac\u0026rdquo;\nClick on \u0026ldquo;System Report\u0026rdquo;\nSelect \u0026ldquo;Graphics/Displays\u0026rdquo; in the sidebar\nVerify NVIDIA Drivers # To check if NVIDIA drivers are installed:\nOpen a terminal and run:\nnvidia-smi This should display information about your GPU, driver version, and CUDA version.\nOpen Command Prompt or PowerShell Run: nvidia-smi Check CUDA Installation # To verify your CUDA installation:\nCheck CUDA version:\nnvcc --version Verify CUDA location:\nls /usr/local/cuda Open Command Prompt or PowerShell\nRun:\nnvcc --version Check CUDA installation directory:\ndir \u0026#34;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\u0026#34; Installing CUDA Drivers # If CUDA is not installed or you need to update it:\nVisit the NVIDIA CUDA Downloads page\nSelect your Linux distribution and version\nFollow the installation instructions provided\nTypically, you\u0026rsquo;ll run commands like:\nwget https://developer.download.nvidia.com/compute/cuda/repos/\u0026lt;distro\u0026gt;/\u0026lt;arch\u0026gt;/cuda-\u0026lt;distro\u0026gt;.pin sudo mv cuda-\u0026lt;distro\u0026gt;.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/\u0026lt;version\u0026gt;/local_installers/cuda-\u0026lt;version\u0026gt;-\u0026lt;arch\u0026gt;.run sudo sh cuda-\u0026lt;version\u0026gt;-\u0026lt;arch\u0026gt;.run Add CUDA to your PATH in ~/.bashrc:\nexport PATH=/usr/local/cuda/bin${PATH:+:${PATH}} export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} Reload your shell:\nsource ~/.bashrc Visit the NVIDIA CUDA Downloads page\nSelect Windows and your version\nDownload and run the installer\nFollow the on-screen instructions\nRestart your computer after installation\nRecent versions of macOS do not support CUDA. For older systems:\nVisit the NVIDIA CUDA Downloads page Select macOS and your version Download and run the installer Follow the on-screen instructions Restart your computer after installation Accessing Compute # To use your NVIDIA GPU for computation:\nCUDA-enabled Applications: Many scientific and machine learning applications are CUDA-enabled and will automatically use your GPU if properly installed.\nProgramming with CUDA:\nUse CUDA C/C++ for direct GPU programming Use libraries like cuDNN for deep learning Use frameworks like PyTorch or TensorFlow, which leverage CUDA Jupyter Notebooks: To check GPU availability in a Jupyter notebook:\nimport torch print(torch.cuda.is_available()) print(torch.cuda.get_device_name(0)) Docker with GPU Support:\nInstall NVIDIA Container Toolkit Run Docker containers with GPU access: docker run --gpus all nvidia/cuda:11.0-base nvidia-smi Cloud GPU Instances:\nUse cloud providers like AWS, GCP, or Azure for GPU instances Install CUDA drivers and libraries on cloud instances Always ensure you\u0026rsquo;re using the correct CUDA version compatible with your GPU and the software you\u0026rsquo;re using.\n"},{"id":5,"href":"/docs/python/production_poetry/","title":"Poetry: Python Dependency Management Made Easy","section":"Python","content":" Poetry # Explanation # Poetry is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you.\nPoetry uses a file called pyproject.toml to declare your project dependencies. It then creates a virtual environment and installs the specified dependencies in that environment. Poetry also locks the versions of your dependencies to ensure reproducibility.\nPoetry simplifies the process of managing Python projects. It handles dependency resolution, virtual environment creation, and package management in one tool. This makes it easier to set up consistent development environments and share your project with others.\nWith Poetry, you can easily:\nManage project dependencies Create and manage virtual environments Build and publish packages Handle dependency conflicts Lock dependencies for reproducible builds Benefits # Dependency Resolution: Automatically resolves and installs project dependencies. Virtual Environment Management: Creates and manages virtual environments for you. Reproducibility: Uses a lock file to ensure consistent installations across different systems. Build and Publish: Simplifies the process of building and publishing Python packages. Installation # Install pyenv:\ncurl https://pyenv.run | bash Set up your shell environment:\nexport PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34; [[ -d $PYENV_ROOT/bin ]] \u0026amp;\u0026amp; export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; Restart your shell:\nsource ~/.bashrc Install Python with pyenv # Install Python 3.10:\npyenv install 3.10 If you encounter errors, install these dependencies:\nsudo apt-get install -y make build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \\ libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl Verify the installation:\npyenv versions Install Poetry # Create a virtual environment and install Poetry:\npython3 -m venv $VENV_PATH $VENV_PATH/bin/pip install -U pip setuptools $VENV_PATH/bin/pip install poetry Create a new project and install dependencies:\npoetry new my-project cd my-project poetry install Add project-specific dependencies:\npoetry add pandas numpy matplotlib seaborn torch torchvision seaborn wandb ipykernel tifffile loguru loky h5py cloudpickle "},{"id":6,"href":"/docs/python/conda_env/","title":"Using Conda Environment","section":"Python","content":" Using Conda Environment # Explanation # Conda is a popular package and environment management system that allows you to create and manage isolated environments for different projects. This guide will walk you through the steps of using Conda to create and manage environments.\nConda works by creating isolated environments that contain their own set of packages and dependencies. This allows you to work on different projects with different requirements without conflicts. Conda also provides a package manager that makes it easy to install, update, and remove packages within an environment.\nConda is a more powerful and flexible tool compared to Python\u0026rsquo;s built-in venv module. It supports multiple programming languages, not just Python, and can manage both Python and non-Python packages. Conda also provides pre-built packages for many scientific computing libraries, making it a popular choice for data science and machine learning projects.\nInstalling Conda # To use Conda, you first need to install it on your system. Follow these steps to install Conda:\nVisit the Conda website and download the appropriate installer for your operating system. Run the installer and follow the instructions to complete the installation process. Install Miniconda for different OS # Windows # (New-Object Net.WebClient).DownloadFile(\u0026#39;https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\u0026#39;, \u0026#39;Miniconda3-latest-Windows-x86_64.exe\u0026#39;) \u0026#34;start Miniconda3-latest-Windows-x86_64.exe /InstallationType=JustMe /RegisterPython=0 /S /D=C:\\Miniconda3\u0026#34; Mac # curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh chmod +x Miniconda3-latest-MacOSX-x86_64.sh bash Miniconda3-latest-MacOSX-x86_64.sh $(HOME)/miniconda3/bin/conda init bash source ~/.bashrc Linux # curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh chmod +x Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh $(HOME)/miniconda3/bin/conda init bash source ~/.bashrc Creating a Conda Environment # Once Conda is installed, you can create a new environment using the following command:\nconda create --name myenv Replace myenv with the desired name for your environment. Conda will create a new environment with the specified name.\nActivating a Conda Environment # To activate a Conda environment, use the following command:\nconda activate myenv Replace myenv with the name of the environment you want to activate. Once activated, any packages you install or commands you run will be isolated within this environment.\nInstalling Packages in a Conda Environment # To install packages in a Conda environment, use the following command:\nconda install package_name Replace package_name with the name of the package you want to install. Conda will automatically resolve dependencies and install the specified package in the active environment.\nListing Conda Environments # To list all the Conda environments available on your system, use the following command:\nconda env list This will display a list of all the environments along with their paths.\nDeactivating a Conda Environment # To deactivate the currently active Conda environment, use the following command:\nconda deactivate This will return you to the base environment.\nRemoving a Conda Environment # To remove a Conda environment, use the following command:\nconda env remove --name myenv Replace myenv with the name of the environment you want to remove. Conda will delete the specified environment and all its associated packages.\n"},{"id":7,"href":"/docs/python/python-virtual-environment/","title":"Using Python Without Breaking Your Local Machine!","section":"Python","content":" Virtual Environment # Explanation # A virtual environment is like a separate workspace for your code. Imagine you have a desk where you do your work, and on that desk, you have different projects spread out. Each project has its own set of tools, materials, and files.\nSimilarly, in programming, a virtual environment is a self-contained space where you can work on different projects without them interfering with each other. It allows you to create an isolated environment with its own set of libraries, dependencies, and configurations.\nWhy is this useful? Well, sometimes different projects require different versions of libraries or dependencies. For example, one project might need an older version of a library, while another project requires a newer version. Without a virtual environment, these projects might conflict with each other and cause issues.\nWith a virtual environment, you can create a separate workspace for each project, ensuring that the specific versions of libraries and dependencies needed for that project are installed and used. This way, you can work on multiple projects simultaneously without worrying about conflicts or compatibility issues.\nBenefits # Project Isolation: No interference between projects. Version Control: Maintain different versions of libraries for different projects. Easy Collaboration: Share your environment setup easily with others. Creating a Python Virtual Environment # Steps to Create a Virtual Environment # Install Python: Make sure Python is installed. Use python3 for Python 3.x.\nCreate a Virtual Environment:\npython3 -m venv myenv Activate the Virtual Environment:\nWindows # myenv\\Scripts\\activate Mac/Linux # source myenv/bin/activate Install Packages:\npip install package_name Deactivate the Environment:\ndeactivate "},{"id":8,"href":"/docs/active-projects/cryosamba/advanced/","title":"From Scratch(Advanced)","section":"Cryosamba","content":" Advanced instructions # Table of Contents # Installation 🐍 Training Setup Training 🛠️ Run Training 🚀 Visualization with TensorBoard 📈 Inference Setup Inference 🛠️ Run Inference 🚀 Installation # Open a terminal window and run conda create --name your-env-name python=3.11 -y to create the environment (replace your-env-name with a desired name). Activate the environment with conda activate your-env-name. In the future, you will have to activate the environment anytime you want to use CryoSamba. Install PyTorch (for CUDA 11.8): pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 Install the remaining libraries: pip install tifffile mrcfile easydict loguru tensorboard cupy-cuda11x Navigate to the directory where you want to save the Cryosamba code (via cd /path/to/dir). Then run git clone https://github.com/kirchhausenlab/Cryosamba.git in this directory. If you only have access to a Cryosamba .zip file instead, simply extract it there.\nTraining # Setup Training # Create a your_train_config.json config file somewhere. Use as default the template below:\n{ \u0026#34;train_dir\u0026#34;: \u0026#34;/path/to/dir/Cryosamba/runs/exp-name/train\u0026#34;, \u0026#34;data_path\u0026#34;: [\u0026#34;/path/to/file/volume.mrc\u0026#34;], \u0026#34;train_data\u0026#34;: { \u0026#34;max_frame_gap\u0026#34;: 6, \u0026#34;patch_shape\u0026#34;: [256, 256], \u0026#34;patch_overlap\u0026#34;: [16, 16], \u0026#34;split_ratio\u0026#34;: 0.95, \u0026#34;batch_size\u0026#34;: 32, \u0026#34;num_workers\u0026#34;: 4 }, \u0026#34;train\u0026#34;: { \u0026#34;load_ckpt_path\u0026#34;: null, \u0026#34;print_freq\u0026#34;: 100, \u0026#34;save_freq\u0026#34;: 1000, \u0026#34;val_freq\u0026#34;: 1000, \u0026#34;num_iters\u0026#34;: 200000, \u0026#34;warmup_iters\u0026#34;: 300, \u0026#34;mixed_precision\u0026#34;: true, \u0026#34;compile\u0026#34;: false }, \u0026#34;optimizer\u0026#34;: { \u0026#34;lr\u0026#34;: 2e-4, \u0026#34;lr_decay\u0026#34;: 0.99995, \u0026#34;weight_decay\u0026#34;: 0.0001, \u0026#34;epsilon\u0026#34;: 1e-8, \u0026#34;betas\u0026#34;: [0.9, 0.999] }, \u0026#34;biflownet\u0026#34;: { \u0026#34;pyr_dim\u0026#34;: 24, \u0026#34;pyr_level\u0026#34;: 3, \u0026#34;corr_radius\u0026#34;: 4, \u0026#34;kernel_size\u0026#34;: 3, \u0026#34;warp_type\u0026#34;: \u0026#34;soft_splat\u0026#34;, \u0026#34;padding_mode\u0026#34;: \u0026#34;reflect\u0026#34;, \u0026#34;fix_params\u0026#34;: false }, \u0026#34;fusionnet\u0026#34;: { \u0026#34;num_channels\u0026#34;: 16, \u0026#34;padding_mode\u0026#34;: \u0026#34;reflect\u0026#34;, \u0026#34;fix_params\u0026#34;: false } } Explanation of parameters:\ntrain_dir: Folder where the checkpoints will be saved (e.g., exp-name/train). data_path: full path to a single (3D) .tif, .mrc or .rec file, or full path to a folder containing a sequence of (2D) .tif files, ordered alphanumerically matching the Z-stack order. You can train on multiple volumes by including the paths as elements of a list. train_data: parameters related to the raw data used for training max_frame_gap: Maximum frame gap used for training (see manuscript). For our data, we used values of 3, 6 and 10 for resolutions of 15.72, 7.86 and 2.62 angstroms/voxel, respectively. patch_shape: X and Y resolution of the patches the model will be trained on (must be a multiple of 32). patch_overlap: overlap (on X and Y) between consecutive patches (see manuscript). split_ratio: train and validation data split ratio. Must be a float between 0 and 1. E.g., 0.95 means that 95% of the data will be assigned for training and 5% for validation. batch_size: Number of data points loaded into the GPU at once. num_workers: Number of simultaneous CPU workers used by the Pytorch Dataloader. train: parameters related to the training routine load_ckpt_path: null to start a training run from scratch, or the path to a (.pt or .pth) model checkpoint if you want to start from a pretrained model. print_freq: frequency of the print statements in number of iterations. save_freq: frequency of model checkpoint saving in number of iterations. val_freq: frequency validation runs in number of iterations. num_iters: Length of the training run (default is 200k iterations). warmup_iters: number of iterations for the learning rate warmu-up. mixed_precision: if true, uses mixed precision training. compile: If true, uses torch.compile for faster training (might lead to errors, and has a few-minutes overhead time before the training iterations). optimizer: parameters related to the optimization algorithm lr: base learning rate. lr_decay: multiplicative factor for the learning rate decay. weight_decay: weight decay for the AdamW optimizer. epsilon: epsilon for the AdamW optimizer. betas: betas for the AdamW optimizer. biflownet: parameters related to the Bi-Directional Optical Flow module (see manuscript and EBME paper) pyr_dim: base number of channels of the Feature Pyramid and Flow Estimator networks\u0026rsquo; layers. pyr_level: number of pyramid levels of the Feature Pyramid network. corr_radius: radius of the correlation volume function. kernel_size: kernel size of the biflownet convolutional layers. warp_type: type of Optical Flow warping (soft_splat, avg_splat, fw_splat or backwarp) padding_mode: padding mode of biflownet convolutional layers. fix_params: set to true in order to fix biflownet\u0026rsquo;s weights and disable learning. fusionnet: num_channels: base number of channels of fusionnet layers. padding_mode: padding mode of fusionnet convolutional layers. fix_params: set to true in order to fix fusionnet\u0026rsquo;s weights and disable learning. Recommended folder structure for each experiment:\nexp-name ├── train └── inference Running a training session may overwrite the exp-name/train folder but won\u0026rsquo;t affect exp-name/inference, and vice versa.\nRun Training # In the terminal, run nvidia-smi to check available GPUs. For example, if you have 8 GPUs they will be numbered from 0 to 7. To train on GPUs 0 and 1, go to the CryoSamba folder and run: CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 train.py --config path/to/your_train_config.json Adjust --nproc_per_node to change the number of GPUs. Use --seed 1234 for reproducibility. To interrupt training, press CTRL + C. You can resume training or start from scratch if prompted. Visualization with TensorBoard # Open a terminal window inside a graphical interface (e.g., a regular desktop computer, Chrome Remote Desktop, XDesk). Activate the environment and run: tensorboard --logdir path/to/exp-name/train In a browser, open localhost:6006. Use the slider under SCALARS to smooth noisy plots. Inference # Setup Inference # Create a your_inference_config.json config file somewhere. Use as default the template below:\n{ \u0026#34;train_dir\u0026#34;: \u0026#34;/path/to/dir/Cryosamba/runs/exp-name/train\u0026#34;, \u0026#34;data_path\u0026#34;: \u0026#34;/path/to/file/volume.mrc\u0026#34;, \u0026#34;inference_dir\u0026#34;: \u0026#34;/path/to/dir/Cryosamba/runs/exp-name/inference\u0026#34;, \u0026#34;inference_data\u0026#34;: { \u0026#34;max_frame_gap\u0026#34;: 12, \u0026#34;patch_shape\u0026#34;: [256, 256], \u0026#34;patch_overlap\u0026#34;: [16, 16], \u0026#34;batch_size\u0026#34;: 32, \u0026#34;num_workers\u0026#34;: 4 }, \u0026#34;inference\u0026#34;: { \u0026#34;output_format\u0026#34;: \u0026#34;same\u0026#34;, \u0026#34;load_ckpt_name\u0026#34;: null, \u0026#34;pyr_level\u0026#34;: 3, \u0026#34;TTA\u0026#34;: true, \u0026#34;mixed_precision\u0026#34;: true, \u0026#34;compile\u0026#34;: true } } Explanation of parameters:\ntrain_dir: Folder from which the checkpoints will be loaded. data_path: full path to a single (3D) .tif, .mrc or .rec file, or full path to a folder containing a sequence of (2D) .tif files, ordered alphanumerically matching the Z-stack order. inference_dir: Folder where the denoised data will be saved (e.g., exp-name/inference). inference_data: parameters related to the raw data used for inference max_frame_gap: maximum frame gap used for inference (see manuscript). For our data, we used values of 6, 12 and 20 for resolutions of 15.72, 7.86 and 2.62 angstroms/voxel, respectively. patch_shape: X and Y resolution of the patches the model will be trained on (must be a multiple of 32). patch_overlap: overlap (on X and Y) between consecutive patches (see manuscript). batch_size: Number of data points loaded into the GPU at once. num_workers: Number of simultaneous CPU workers used by the Pytorch Dataloader. inference: parameters related to the inference routine output_format: \u0026quot;same\u0026quot; to save the denoised result in the same format as the input raw data. Otherwise, specify either \u0026quot;tif_file\u0026quot;, \u0026quot;mrc_file\u0026quot;, \u0026quot;rec_file\u0026quot; or \u0026quot;tif_sequence\u0026quot;. load_ckpt_path: null to load model weights from train-dir/last.pt, otherwise use the path for a custom model checkpoint. pyr_level: number of pyramid levels of the Feature Pyramid network of the biflownet. TTA: if true, uses Test-Time Augmentation (see manuscript), for slightly better results at the cost of longer inference times. mixed_precision: if true, uses mixed precision training. compile: If true, uses torch.compile for faster inference (might lead to errors, and has a few-minutes overhead time before the inference iterations). Run Inference # In the terminal, run nvidia-smi to check available GPUs. For example, if you have 8 GPUs they will be numbered from 0 to 7. To run inference on GPUs 0 and 1, go to the CryoSamba folder and run: CUDA_VISIBLE_DEVICES=0,1 torchrun --standalone --nproc_per_node=2 inference.py --config path/to/your_inference_config.json Adjust --nproc_per_node to change the number of GPUs. Use --seed 1234 for reproducibility. To interrupt inference, press CTRL + C. You can resume or start from scratch if prompted. The final denoised volume will be located at /path/to/dir/runs/exp-name/inference. It will be either a file named result.tif, result.mrc, result.rec or a folder named result. "},{"id":9,"href":"/docs/labcomputingsite/labcomputingsite/","title":"Lab Computing Site","section":"Lab Computing Site","content":" Lab Computing Site # More Information On this Website\n"},{"id":10,"href":"/docs/sbgrid_wiki/sbgrid/","title":"SBGrid Wiki","section":"Sbgrid Wiki","content":" SBGrid Wiki # They support many labs, who all use the suite of tools differently. Much of the wiki is dedicated to support for labs that use the SBGrid GUI and program browser, but there is still useful information for our lab When you encounter an IT issue, the best thing to do is create a help ticket by emailing help@sbgrid.org. Nine times of ten, the person responding will be Justin O’Connor, who dedicates extra time from his schedule to managing TK Lab computing infrastructure. Code managed by you as a user (such as a GitHub repo you install and build from source) will be in your home folder. But software packages such as MATLAB, will be accessed from an SBGrid network installation managed by SBGrid. This ensures version parity when people access the same program from many different computers.\n"}]